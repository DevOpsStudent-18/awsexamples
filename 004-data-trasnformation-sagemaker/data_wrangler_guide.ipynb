{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1932a816",
   "metadata": {},
   "source": [
    "# Data Transformation using Amazon SageMaker Data Wrangler\n",
    "\n",
    "This guide demonstrates how to perform data transformation using **Amazon SageMaker Data Wrangler**, following similar steps to the [previous module](./../003-data-transformation/data_transformation.ipynb), but using the Data Wrangler UI instead of Jupyter notebooks.\n",
    "\n",
    "## âœ… Prerequisites\n",
    "- AWS account with SageMaker access\n",
    "- `mock_data.csv` (located in the `data` folder of this repo)\n",
    "- An S3 bucket to store your data\n",
    "\n",
    "## ðŸš€ Step-by-Step Instructions\n",
    "\n",
    "### 1. Upload Data to S3\n",
    "1. Go to the [AWS S3 Console](https://s3.console.aws.amazon.com/).\n",
    "2. Choose or create a new S3 bucket.\n",
    "3. Upload the `mock_data.csv` file into this bucket.\n",
    "\n",
    "### 2. Set Up SageMaker Studio Environment\n",
    "1. Go to the [AWS SageMaker Console](https://console.aws.amazon.com/sagemaker/).\n",
    "2. Navigate to **\"Domains\"** under **Admin configurations**.\n",
    "3. Click **Create Domain**, use the default settings, and click **Create**.\n",
    "4. After the domain is created, click on the domain name.\n",
    "5. Go to **User Profiles**, create a new profile (if not already created).\n",
    "6. From the user profile, click **Launch app > Studio**.\n",
    "\n",
    "### 3. Open Data Wrangler\n",
    "1. In SageMaker Studio, on the left sidebar, click on the **Data** icon.\n",
    "2. Select the **Data Wrangler** tab.\n",
    "3. Click **Start Canvas**, then **Open in Canvas** to launch Data Wrangler.\n",
    "\n",
    "### 4. Import Dataset\n",
    "1. Click **Import and prepare data**.\n",
    "2. Select **Dataset type** as **Tabular**.\n",
    "3. Choose **S3** as your data source.\n",
    "4. Locate and select the `mock_data.csv` file.\n",
    "5. Click **Import** to load the dataset into Data Wrangler.\n",
    "6. Rename the default flow if desired (from the top of the interface).\n",
    "\n",
    "### 5. Apply Data Cleaning & Transformation\n",
    "Click **Transform** and apply the following steps:\n",
    "\n",
    "- âœ… **Remove duplicates**\n",
    "- âœ… **Handle missing values**:\n",
    "  - Fill `age` with **mean**\n",
    "  - Fill `salary` with **median**\n",
    "  - Fill `department` with **\"Unknown\"**\n",
    "- âœ… **Parse `profile` column** into:\n",
    "  - `address`, `phone`, `email`\n",
    "- âœ… **Rename** the extracted columns appropriately\n",
    "- âœ… **Drop the original `profile` column**\n",
    "\n",
    "> After each transformation, click **Apply** to save the change.\n",
    "\n",
    "### 6. Export Cleaned Dataset\n",
    "1. Click **Export** from the toolbar.\n",
    "2. Select **Amazon S3** as the export destination.\n",
    "3. Choose the output S3 bucket and specify a folder path.\n",
    "4. Click **Export**.\n",
    "\n",
    "### 7. Feature Engineering\n",
    "Use **Custom Formula Transformations** to add the following:\n",
    "\n",
    "- âœ… **`address_length`**:\n",
    "  ```sql\n",
    "  IF(address IS NOT NULL, length(cast(address AS string)), 0)\n",
    "  ```\n",
    "\n",
    "- âœ… **`salary_category`**:\n",
    "  ```sql\n",
    "  CASE\n",
    "      WHEN salary IS NULL THEN 'unknown'\n",
    "      WHEN salary <= 50000 THEN 'low'\n",
    "      WHEN salary > 50000 AND salary <= 70000 THEN 'medium'\n",
    "      WHEN salary > 70000 AND salary <= 100000 THEN 'high'\n",
    "      ELSE 'very high'\n",
    "  END\n",
    "  ```\n",
    "\n",
    "- âœ… **`age_group`**:\n",
    "  ```sql\n",
    "  CASE\n",
    "      WHEN age IS NULL THEN 'Unknown'\n",
    "      WHEN age <= 25 THEN 'Young'\n",
    "      WHEN age > 25 AND age <= 35 THEN 'Early Career'\n",
    "      WHEN age > 35 AND age <= 45 THEN 'Mid Career'\n",
    "      WHEN age > 45 AND age <= 55 THEN 'Senior'\n",
    "      ELSE 'Experienced'\n",
    "  END\n",
    "  ```\n",
    "\n",
    "> After each formula transformation, click **Apply**.\n",
    "\n",
    "### 8. Export Final Dataset\n",
    "1. Click **Export** again after feature engineering.\n",
    "2. Choose the **same or a new S3 bucket**.\n",
    "3. Provide an output path for the final transformed dataset.\n",
    "4. Click **Export** to complete.\n",
    "\n",
    "## ðŸ§¹ Cleanup Resources\n",
    "To avoid unwanted charges:\n",
    "\n",
    "1. Delete the S3 bucket (if no longer needed).\n",
    "2. In SageMaker Studio, go to **Data Wrangler > Canvas** and click **Stop**.\n",
    "3. Delete the SageMaker **Domain** and **User Profile** (if unused).\n",
    "4. Optionally, delete the entire **SageMaker Studio** environment.\n",
    "\n",
    "## âœ… Youâ€™re Done!\n",
    "You've successfully set up Amazon SageMaker Data Wrangler, transformed your dataset, performed feature engineering, and exported your final dataset for further use in ML pipelines or analysis.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}