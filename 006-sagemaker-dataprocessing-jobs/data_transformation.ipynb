{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91601180-d000-42e2-8982-3cdd41d575f0",
   "metadata": {},
   "source": [
    "# Data Transformation Pipeline for MLOps Using SageMaker Data Processing Jobs\n",
    "**Author: Anvesh Muppeda**\n",
    "\n",
    "`This notebook demonstrates a comprehensive data transformation pipeline using Amazon SageMaker Processing Jobs. It showcases MLOps best practices for data preprocessing, feature engineering, and quality monitoring in a production-ready environment. It is designed to be run in a SageMaker Jupyter Notebook environment, leveraging SageMaker's data processing capabilities for efficient data handling and transformation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880256bb-d076-4060-adaa-ad2fc830af7d",
   "metadata": {},
   "source": [
    "## 1. Prepare Environment\n",
    "### üì¶ Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e973b107-b357-4dd0-b794-b04a0f3e3c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:44:21.388306Z",
     "iopub.status.busy": "2025-07-20T23:44:21.387748Z",
     "iopub.status.idle": "2025-07-20T23:44:21.690050Z",
     "shell.execute_reply": "2025-07-20T23:44:21.689345Z",
     "shell.execute_reply.started": "2025-07-20T23:44:21.388278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role: arn:aws:iam::910316760829:role/service-role/AmazonSageMaker-ExecutionRole-20250720T171468\n",
      "Default Bucket: sagemaker-us-east-1-910316760829\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Setup Environment\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9baa537-063a-44cd-a4c2-6aa4366a9998",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Step 2: Data Generation\n",
    "Creating a realistic dataset that simulates common data quality challenges found in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb4676e5-804d-4d6e-8781-b03b8092be7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:44:21.691618Z",
     "iopub.status.busy": "2025-07-20T23:44:21.691130Z",
     "iopub.status.idle": "2025-07-20T23:44:22.009117Z",
     "shell.execute_reply": "2025-07-20T23:44:22.008223Z",
     "shell.execute_reply.started": "2025-07-20T23:44:21.691595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and uploaded to data/mock_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records\n",
    "num_records = 20000\n",
    "\n",
    "# Generate random data\n",
    "data = {\n",
    "    \"id\": np.arange(1, num_records + 1),\n",
    "    \"name\": [f\"Name_{i}\" for i in np.random.randint(1, 1000, num_records)],\n",
    "    \"age\": np.random.randint(18, 80, num_records),\n",
    "    \"salary\": np.random.choice([50000, 60000, 70000, None], num_records),\n",
    "    \"hire_date\": [\n",
    "        (datetime.now() - timedelta(days=random.randint(0, 3650))).strftime(\"%Y-%m-%d\")\n",
    "        if random.random() > 0.1 else None\n",
    "        for _ in range(num_records)\n",
    "    ],\n",
    "    \"profile\": [\n",
    "        json.dumps({\n",
    "            \"address\": f\"Street {random.randint(1, 100)}, City {random.randint(1, 50)}\",\n",
    "            \"phone\": f\"{random.randint(1000000000, 9999999999)}\",\n",
    "            \"email\": f\"email_{random.randint(1, 1000)}@example.com\"\n",
    "        })\n",
    "        if random.random() > 0.1 else None\n",
    "        for _ in range(num_records)\n",
    "    ],\n",
    "    \"department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", None], num_records),\n",
    "    \"bonus\": [None if random.random() > 0.9 else random.randint(1000, 10000) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some NaN values randomly\n",
    "df.loc[np.random.choice(df.index, size=int(num_records * 0.05), replace=False), \"age\"] = np.nan\n",
    "df.loc[np.random.choice(df.index, size=int(num_records * 0.1), replace=False), \"salary\"] = np.nan\n",
    "\n",
    "# Ensure 'data' folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"data/mock_data.csv\", index=False)\n",
    "print(\"Dataset created and uploaded to data/mock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d5e28-a7dc-40f4-8767-6ad631e3e17d",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Step 3: Upload Source Data to S3\n",
    "Upload the source CSV dataset to input location in S3 (default bucket)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbbf0e65-caee-417a-bddc-f8533c6c613c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:44:22.010925Z",
     "iopub.status.busy": "2025-07-20T23:44:22.010579Z",
     "iopub.status.idle": "2025-07-20T23:44:22.134620Z",
     "shell.execute_reply": "2025-07-20T23:44:22.133882Z",
     "shell.execute_reply.started": "2025-07-20T23:44:22.010877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mock_data.csv' uploaded to: s3://sagemaker-us-east-1-910316760829/input/mock_data.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('data/mock_data.csv', bucket, 'input/mock_data.csv')\n",
    "print(f\"Dataset 'mock_data.csv' uploaded to: s3://{bucket}/input/mock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781eaf7-4980-43c7-8608-746ad3666a94",
   "metadata": {},
   "source": [
    "## 2. Data Processing Job. \n",
    "Here we will create a SageMaker Processing Job to execute the data transformation script. This job will handle the data preprocessing, feature engineering, and quality checks.\n",
    "### Step 1: üõ†Ô∏è Create Processing Script\n",
    "This script contains all the data transformation logic that will be executed by SageMaker Processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ba1bd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:44:22.136979Z",
     "iopub.status.busy": "2025-07-20T23:44:22.136629Z",
     "iopub.status.idle": "2025-07-20T23:44:22.149270Z",
     "shell.execute_reply": "2025-07-20T23:44:22.146904Z",
     "shell.execute_reply.started": "2025-07-20T23:44:22.136954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing_script.py\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv('/opt/ml/processing/input/mock_data.csv')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìè Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: mock_data.csv not found. Please run create_dataset.py first.\")\n",
    "    exit()\n",
    "\n",
    "# Analyze missing patterns\n",
    "print(\"\\nüìä Missing Value Patterns:\")\n",
    "print(\"Missing Age values:\")\n",
    "print(df[df['age'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "print(\"Missing Salary values\")\n",
    "print(df[df['salary'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "# Get the median values for age, and salary\n",
    "age_median = df['age'].median()\n",
    "salary_median = df['salary'].median()\n",
    "print(\"Age Median\", age_median)\n",
    "print(\"Salary Median\", salary_median)\n",
    "\n",
    "# Fill missing values of age with age_median\n",
    "df['age'] = df['age'].fillna(age_median)\n",
    "# Fill missing values of salary with salary_median\n",
    "df['salary'] = df['salary'].fillna(salary_median)\n",
    "\n",
    "# Verify the Age & Salary data\n",
    "df.head()\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column\")\n",
    "df.isnull().sum()\n",
    "\n",
    "print(\"Print the missing values for Department\\n\")\n",
    "print(\"Missing Department Missing values\")\n",
    "print(df[df['department'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "# Fill the missing values in department with 'Unknown'\n",
    "df['department'] = df['department'].fillna('Unknown')\n",
    "\n",
    "# Verify the Age & Salary data\n",
    "df.head()\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column\")\n",
    "print(df.isnull().sum())\n",
    "# Check unique values in the department column\n",
    "df['department'].unique()\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "\n",
    "# Find the first non-null value in the column\n",
    "profile_first_value = df['profile'].dropna().iloc[0]\n",
    "# Print its type\n",
    "print(\"\\nProfile column values current data type\")\n",
    "print(type(profile_first_value))\n",
    "\n",
    "# If your 'profile' column already contains Python dictionaries, not JSON strings.\n",
    "# You do not need to parse it with json.loads(). The data is ready to be used directly.\n",
    "\n",
    "# Convert profile JSON strings into dictionaries\n",
    "df['profile'] = df['profile'].apply(lambda x: json.loads(x) if pd.notnull(x) else {})\n",
    "\n",
    "# Extract Address Field\n",
    "print(\"Extract Address Field....\\n\")\n",
    "# Create new 'address' column by extracting from 'profile' dictionaries\n",
    "df['address'] = df['profile'].apply(lambda x: x.get('address', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created address column \\n\")\n",
    "print(df['address'].head())\n",
    "\n",
    "# Extract Phone Field\n",
    "print(\"Extract Phone Field....\\n\")\n",
    "# Create new 'phone' column by extracting from 'profile' dictionaries\n",
    "df['phone'] = df['profile'].apply(lambda x: x.get('phone', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created phone column \\n\")\n",
    "print(df['phone'].head())\n",
    "\n",
    "# Extract Email Field\n",
    "print(\"Extract Email Field....\\n\")\n",
    "# Create new 'email' column by extracting from 'profile' dictionaries\n",
    "df['email'] = df['profile'].apply(lambda x: x.get('email', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created email column \\n\")\n",
    "print(df['email'].head())\n",
    "\n",
    "print(f\"\\n‚úÖ Profile fields extracted:\")\n",
    "\n",
    "\n",
    "# Now drop the profile column\n",
    "print(\"\\nColumns before dropping profile:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Without inplace=True (df remains unchanged)\n",
    "cleaned_df = df.drop(columns=['profile'])\n",
    "\n",
    "# With inplace=True (df is modified directly)\n",
    "#df.drop(columns=['profile'], inplace=True)\n",
    "\n",
    "print(\"\\nColumns in new DataFrame after dropping profile:\")\n",
    "# print(df.columns.tolist())\n",
    "print(cleaned_df.columns.tolist())\n",
    "\n",
    "print(\"\\nüíæ Saving cleaned data to: 'data/cleaned_data.csv' ...\")\n",
    "cleaned_df.to_csv(\"/opt/ml/processing/output/cleaned_data.csv\", index=False)\n",
    "print(\"‚úÖ Cleaned data saved to: '/opt/ml/processing/output/cleaned_data.csv'\")\n",
    "\n",
    "transform_df = pd.read_csv('/opt/ml/processing/output/cleaned_data.csv')\n",
    "transform_df.head()\n",
    "\n",
    "# Create a new column 'address_length' \n",
    "print(\"\\nüîß Creating Address Length Feature...\")\n",
    "transform_df['address_length'] = transform_df['address'].apply(lambda x: len(str(x)))\n",
    "print(\"Address followed by Address Length columns\")\n",
    "transform_df[['address', 'address_length']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Salary Categories...\")\n",
    "# Define the bins and labels\n",
    "bins = [0, 50000, 70000, 100000]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# Create a new column 'salary_category'\n",
    "transform_df['salary_category'] = pd.cut(df['salary'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Print sample data after adding the 'salary_category' column\n",
    "print(\"Sample data after adding the 'salary_category' column: \\n\")\n",
    "transform_df[['salary', 'salary_category']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Age Groups...\")\n",
    "# Define age bins and labels\n",
    "age_bins = [0, 25, 35, 45, 55, float('inf')]\n",
    "age_labels = ['Young', 'Early Career', 'Mid Career', 'Senior', 'Experienced']\n",
    "\n",
    "# Create a new column 'salary_category'\n",
    "transform_df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "\n",
    "# Age group distribution\n",
    "print(f\"Age group distribution:\")\n",
    "print(transform_df['age_group'].value_counts())\n",
    "\n",
    "# Print sample data after adding the 'salary_category' column\n",
    "print(\"\\nSample data after adding the 'age_group' column: \\n\")\n",
    "transform_df[['age', 'age_group']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Department Statistics...\")\n",
    "# Group by 'department' and calculate average salary and age\n",
    "department_summary_report = df.groupby('department').agg({\n",
    "    'salary': 'mean',\n",
    "    'age': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename columns of department_summary_report for clarity\n",
    "department_summary_report.columns = ['Department', 'Average Salary', 'Average Age']\n",
    "\n",
    "# Print the Summary Report\n",
    "print(\"Summary report of average salary and age based on the department:\\n\")\n",
    "print(department_summary_report)\n",
    "\n",
    "\n",
    "print(\"\\nüìä Data Quality Metrics...\")\n",
    "\n",
    "quality_metrics = {\n",
    "    'total_rows': len(transform_df),\n",
    "    'total_columns': len(transform_df.columns),\n",
    "    'missing_values_count': transform_df.isnull().sum().sum(),\n",
    "    'duplicate_rows': transform_df.duplicated().sum(),\n",
    "    'numeric_columns': len(transform_df.select_dtypes(include=[np.number]).columns),\n",
    "    'categorical_columns': len(transform_df.select_dtypes(include=['object']).columns),\n",
    "    'unique_departments': transform_df['department'].nunique(),\n",
    "    'unique_age_groups': transform_df['age_group'].nunique(),\n",
    "    'unique_salary_categories': transform_df['salary_category'].nunique(),\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"Data Quality Metrics:\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(\"Saving Transformed data csv to: '/opt/ml/processing/output/transformed_data.csv' ...\")\n",
    "transform_df.to_csv(\"/opt/ml/processing/output/transformed_data.csv\", index=False)\n",
    "print(\"\\nTransformed data csv saved to: '/opt/ml/processing/output/transformed_data.csv'\")\n",
    "\n",
    "### Step 2: Save Department Statistics\n",
    "print(\"Saving department statistics...\")\n",
    "department_summary_report.to_csv(\"/opt/ml/processing/output/department_statistics.csv\", index=False)\n",
    "print(\"‚úÖ Department statistics saved to: '/opt/ml/processing/output/department_statistics.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cef80-1465-4bf7-a5dd-77c98d726590",
   "metadata": {},
   "source": [
    "### Step 2: üèÉ‚Äç‚ôÇÔ∏è Execute SageMaker Processing Job\n",
    "Now we will execute the SageMaker Processing Job using the script created in the previous step. This job will process the data, apply transformations, and generate output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8314a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:44:22.150923Z",
     "iopub.status.busy": "2025-07-20T23:44:22.150365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2025-07-20-23-44-22-210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    }
   ],
   "source": [
    "input_raw_data_prefix = \"input/\"\n",
    "output_preprocessed_data_prefix = \"output\"\n",
    "\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve('sklearn', 'us-east-1', '1.2-1'),\n",
    "    role=role,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.t3.medium',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code='preprocessing_script.py',\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"mock_data.csv\"),\n",
    "                            destination='/opt/ml/processing/input')], \n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                            destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"data-processed\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae444d6b",
   "metadata": {},
   "source": [
    "## 3. Results Validation\n",
    "üìä Review Processed Data.  \n",
    "Validating the results of our data processing pipeline to ensure quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb859e-1854-4ae2-9bfa-f9ecf865bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(f's3://{bucket}/output/data-processed/cleaned_data.csv')\n",
    "df_department_statistics = pd.read_csv(f's3://{bucket}/output/data-processed/department_statistics.csv')\n",
    "df_transformed_data = pd.read_csv(f's3://{bucket}/output/data-processed/transformed_data.csv')\n",
    "\n",
    "print(\"üßπ CLEANED DATA SAMPLE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Cleaned Data..\\n\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "print(\"\\nüîß TRANSFORMED DATA SAMPLE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Trnasformed Data..\\n\")\n",
    "print(df_transformed_data.head())\n",
    "\n",
    "print(\"\\nüìà DEPARTMENT STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Department Statistics Data..\\n\")\n",
    "print(df_department_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2d6f8-27a3-43ad-9e83-fdbf92a2fe23",
   "metadata": {},
   "source": [
    "## 4. Next Steps for MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab1b76-7e48-4105-8720-f35ed5584607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ RECOMMENDED NEXT STEPS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. ü§ñ Model Training:\")\n",
    "print(\"   - Use transformed features for ML model training\")\n",
    "print(\"   - Implement cross-validation with engineered features\")\n",
    "print(\"   - Track model performance metrics\")\n",
    "\n",
    "print(\"\\n2. üß™ Model Validation:\")\n",
    "print(\"   - A/B test model performance with/without new features\")\n",
    "print(\"   - Validate model predictions on hold-out dataset\")\n",
    "print(\"   - Monitor feature importance and model interpretability\")\n",
    "\n",
    "print(\"\\n3. üöÄ Model Deployment:\")\n",
    "print(\"   - Deploy model using SageMaker endpoints\")\n",
    "print(\"   - Implement batch or real-time inference\")\n",
    "print(\"   - Set up model versioning and rollback capabilities\")\n",
    "\n",
    "print(\"\\n4. üìä Monitoring & Observability:\")\n",
    "print(\"   - Monitor data drift using quality metrics\")\n",
    "print(\"   - Set up alerts for data quality degradation\")\n",
    "print(\"   - Track model performance in production\")\n",
    "\n",
    "print(\"\\n5. üîÑ Pipeline Automation:\")\n",
    "print(\"   - Integrate with SageMaker Pipelines for orchestration\")\n",
    "print(\"   - Schedule regular data processing jobs\")\n",
    "print(\"   - Implement CI/CD for model updates\")\n",
    "\n",
    "print(\"\\n6. üìà Advanced Analytics:\")\n",
    "print(\"   - Use department statistics for business insights\")\n",
    "print(\"   - Create dashboards for data quality monitoring\")\n",
    "print(\"   - Implement anomaly detection on incoming data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ DATA TRANSFORMATION PIPELINE COMPLETE!\")\n",
    "print(\"üìÅ Processed data ready for ML model training\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
