{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91601180-d000-42e2-8982-3cdd41d575f0",
   "metadata": {},
   "source": [
    "# Data Transformation Pipeline for MLOps Using SageMaker Data Processing Jobs\n",
    "**Author: Anvesh Muppeda**\n",
    "\n",
    "`This notebook demonstrates a comprehensive data transformation pipeline using Amazon SageMaker Processing Jobs. It showcases MLOps best practices for data preprocessing, feature engineering, and quality monitoring in a production-ready environment. It is designed to be run in a SageMaker Jupyter Notebook environment, leveraging SageMaker's data processing capabilities for efficient data handling and transformation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880256bb-d076-4060-adaa-ad2fc830af7d",
   "metadata": {},
   "source": [
    "## 1. Prepare Environment\n",
    "### üì¶ Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e973b107-b357-4dd0-b794-b04a0f3e3c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T22:48:59.744467Z",
     "iopub.status.busy": "2025-07-20T22:48:59.744129Z",
     "iopub.status.idle": "2025-07-20T22:49:03.738733Z",
     "shell.execute_reply": "2025-07-20T22:49:03.737884Z",
     "shell.execute_reply.started": "2025-07-20T22:48:59.744386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Role: arn:aws:iam::910316760829:role/service-role/AmazonSageMaker-ExecutionRole-20250720T171468\n",
      "Default Bucket: sagemaker-us-east-1-910316760829\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Setup Environment\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9baa537-063a-44cd-a4c2-6aa4366a9998",
   "metadata": {},
   "source": [
    "### Step 2: Data Generation\n",
    "Creating a realistic dataset that simulates common data quality challenges found in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4676e5-804d-4d6e-8781-b03b8092be7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T22:49:03.743344Z",
     "iopub.status.busy": "2025-07-20T22:49:03.742724Z",
     "iopub.status.idle": "2025-07-20T22:49:04.098179Z",
     "shell.execute_reply": "2025-07-20T22:49:04.097336Z",
     "shell.execute_reply.started": "2025-07-20T22:49:03.743293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and uploaded to data/mock_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records\n",
    "num_records = 20000\n",
    "\n",
    "# Generate random data\n",
    "data = {\n",
    "    \"id\": np.arange(1, num_records + 1),\n",
    "    \"name\": [f\"Name_{i}\" for i in np.random.randint(1, 1000, num_records)],\n",
    "    \"age\": np.random.randint(18, 80, num_records),\n",
    "    \"salary\": np.random.choice([50000, 60000, 70000, None], num_records),\n",
    "    \"hire_date\": [\n",
    "        (datetime.now() - timedelta(days=random.randint(0, 3650))).strftime(\"%Y-%m-%d\")\n",
    "        if random.random() > 0.1 else None\n",
    "        for _ in range(num_records)\n",
    "    ],\n",
    "    \"profile\": [\n",
    "        json.dumps({\n",
    "            \"address\": f\"Street {random.randint(1, 100)}, City {random.randint(1, 50)}\",\n",
    "            \"phone\": f\"{random.randint(1000000000, 9999999999)}\",\n",
    "            \"email\": f\"email_{random.randint(1, 1000)}@example.com\"\n",
    "        })\n",
    "        if random.random() > 0.1 else None\n",
    "        for _ in range(num_records)\n",
    "    ],\n",
    "    \"department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", None], num_records),\n",
    "    \"bonus\": [None if random.random() > 0.9 else random.randint(1000, 10000) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some NaN values randomly\n",
    "df.loc[np.random.choice(df.index, size=int(num_records * 0.05), replace=False), \"age\"] = np.nan\n",
    "df.loc[np.random.choice(df.index, size=int(num_records * 0.1), replace=False), \"salary\"] = np.nan\n",
    "\n",
    "# Ensure 'data' folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"data/mock_data.csv\", index=False)\n",
    "print(\"Dataset created and uploaded to data/mock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d5e28-a7dc-40f4-8767-6ad631e3e17d",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Step 3: Upload Source Data to S3\n",
    "Upload the source CSV dataset to input location in S3 (default bucket)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbf0e65-caee-417a-bddc-f8533c6c613c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T22:49:04.099648Z",
     "iopub.status.busy": "2025-07-20T22:49:04.099340Z",
     "iopub.status.idle": "2025-07-20T22:49:04.340455Z",
     "shell.execute_reply": "2025-07-20T22:49:04.339627Z",
     "shell.execute_reply.started": "2025-07-20T22:49:04.099618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mock_data.csv' uploaded to: s3://sagemaker-us-east-1-910316760829/input/mock_data.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('data/mock_data.csv', bucket, 'input/mock_data.csv')\n",
    "print(f\"Dataset 'mock_data.csv' uploaded to: s3://{bucket}/input/mock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781eaf7-4980-43c7-8608-746ad3666a94",
   "metadata": {},
   "source": [
    "## 2. Data Processing Job. \n",
    "Here we will create a SageMaker Processing Job to execute the data transformation script. This job will handle the data preprocessing, feature engineering, and quality checks.\n",
    "### Step 1: üõ†Ô∏è Create Processing Script\n",
    "This script contains all the data transformation logic that will be executed by SageMaker Processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1bd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:04:38.292094Z",
     "iopub.status.busy": "2025-07-20T23:04:38.291723Z",
     "iopub.status.idle": "2025-07-20T23:04:38.299871Z",
     "shell.execute_reply": "2025-07-20T23:04:38.299008Z",
     "shell.execute_reply.started": "2025-07-20T23:04:38.292070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing_script.py\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv('/opt/ml/processing/input/mock_data.csv')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìè Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: mock_data.csv not found. Please run create_dataset.py first.\")\n",
    "    exit()\n",
    "\n",
    "# Analyze missing patterns\n",
    "print(\"\\nüìä Missing Value Patterns:\")\n",
    "print(\"Missing Age values:\")\n",
    "print(df[df['age'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "print(\"Missing Salary values\")\n",
    "print(df[df['salary'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "# Get the median values for age, and salary\n",
    "age_median = df['age'].median()\n",
    "salary_median = df['salary'].median()\n",
    "print(\"Age Median\", age_median)\n",
    "print(\"Salary Median\", salary_median)\n",
    "\n",
    "# Fill missing values of age with age_median\n",
    "df['age'] = df['age'].fillna(age_median)\n",
    "# Fill missing values of salary with salary_median\n",
    "df['salary'] = df['salary'].fillna(salary_median)\n",
    "\n",
    "# Verify the Age & Salary data\n",
    "df.head()\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column\")\n",
    "df.isnull().sum()\n",
    "\n",
    "print(\"Print the missing values for Department\\n\")\n",
    "print(\"Missing Department Missing values\")\n",
    "print(df[df['department'].isnull()][['age', 'salary', 'department']])\n",
    "\n",
    "# Fill the missing values in department with 'Unknown'\n",
    "df['department'] = df['department'].fillna('Unknown')\n",
    "\n",
    "# Verify the Age & Salary data\n",
    "df.head()\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column\")\n",
    "print(df.isnull().sum())\n",
    "# Check unique values in the department column\n",
    "df['department'].unique()\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "\n",
    "# Find the first non-null value in the column\n",
    "profile_first_value = df['profile'].dropna().iloc[0]\n",
    "# Print its type\n",
    "print(\"\\nProfile column values current data type\")\n",
    "print(type(profile_first_value))\n",
    "\n",
    "# If your 'profile' column already contains Python dictionaries, not JSON strings.\n",
    "# You do not need to parse it with json.loads(). The data is ready to be used directly.\n",
    "\n",
    "# Convert profile JSON strings into dictionaries\n",
    "df['profile'] = df['profile'].apply(lambda x: json.loads(x) if pd.notnull(x) else {})\n",
    "\n",
    "# Extract Address Field\n",
    "print(\"Extract Address Field....\\n\")\n",
    "# Create new 'address' column by extracting from 'profile' dictionaries\n",
    "df['address'] = df['profile'].apply(lambda x: x.get('address', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created address column \\n\")\n",
    "print(df['address'].head())\n",
    "\n",
    "# Extract Phone Field\n",
    "print(\"Extract Phone Field....\\n\")\n",
    "# Create new 'phone' column by extracting from 'profile' dictionaries\n",
    "df['phone'] = df['profile'].apply(lambda x: x.get('phone', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created phone column \\n\")\n",
    "print(df['phone'].head())\n",
    "\n",
    "# Extract Email Field\n",
    "print(\"Extract Email Field....\\n\")\n",
    "# Create new 'email' column by extracting from 'profile' dictionaries\n",
    "df['email'] = df['profile'].apply(lambda x: x.get('email', None))  # Returns None if no address key\n",
    "\n",
    "print(\"Top rows from profile column \\n\")\n",
    "print(df['profile'].head())\n",
    "print(\"\\nTop rows from newly created email column \\n\")\n",
    "print(df['email'].head())\n",
    "\n",
    "print(f\"\\n‚úÖ Profile fields extracted:\")\n",
    "\n",
    "\n",
    "# Now drop the profile column\n",
    "print(\"\\nColumns before dropping profile:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Without inplace=True (df remains unchanged)\n",
    "cleaned_df = df.drop(columns=['profile'])\n",
    "\n",
    "# With inplace=True (df is modified directly)\n",
    "#df.drop(columns=['profile'], inplace=True)\n",
    "\n",
    "print(\"\\nColumns in new DataFrame after dropping profile:\")\n",
    "# print(df.columns.tolist())\n",
    "print(cleaned_df.columns.tolist())\n",
    "\n",
    "print(\"\\nüíæ Saving cleaned data to: 'data/cleaned_data.csv' ...\")\n",
    "cleaned_df.to_csv(\"/opt/ml/processing/output/cleaned_data.csv\", index=False)\n",
    "print(\"‚úÖ Cleaned data saved to: '/opt/ml/processing/output/cleaned_data.csv'\")\n",
    "\n",
    "transform_df = pd.read_csv('/opt/ml/processing/output/cleaned_data.csv')\n",
    "transform_df.head()\n",
    "\n",
    "# Create a new column 'address_length' \n",
    "print(\"\\nüîß Creating Address Length Feature...\")\n",
    "transform_df['address_length'] = transform_df['address'].apply(lambda x: len(str(x)))\n",
    "print(\"Address followed by Address Length columns\")\n",
    "transform_df[['address', 'address_length']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Salary Categories...\")\n",
    "# Define the bins and labels\n",
    "bins = [0, 50000, 70000, 100000]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# Create a new column 'salary_category'\n",
    "transform_df['salary_category'] = pd.cut(df['salary'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Print sample data after adding the 'salary_category' column\n",
    "print(\"Sample data after adding the 'salary_category' column: \\n\")\n",
    "transform_df[['salary', 'salary_category']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Age Groups...\")\n",
    "# Define age bins and labels\n",
    "age_bins = [0, 25, 35, 45, 55, float('inf')]\n",
    "age_labels = ['Young', 'Early Career', 'Mid Career', 'Senior', 'Experienced']\n",
    "\n",
    "# Create a new column 'salary_category'\n",
    "transform_df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "\n",
    "# Age group distribution\n",
    "print(f\"Age group distribution:\")\n",
    "print(transform_df['age_group'].value_counts())\n",
    "\n",
    "# Print sample data after adding the 'salary_category' column\n",
    "print(\"\\nSample data after adding the 'age_group' column: \\n\")\n",
    "transform_df[['age', 'age_group']].head()\n",
    "\n",
    "print(\"\\nüîß Creating Department Statistics...\")\n",
    "# Group by 'department' and calculate average salary and age\n",
    "department_summary_report = df.groupby('department').agg({\n",
    "    'salary': 'mean',\n",
    "    'age': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename columns of department_summary_report for clarity\n",
    "department_summary_report.columns = ['Department', 'Average Salary', 'Average Age']\n",
    "\n",
    "# Print the Summary Report\n",
    "print(\"Summary report of average salary and age based on the department:\\n\")\n",
    "print(department_summary_report)\n",
    "\n",
    "\n",
    "print(\"\\nüìä Data Quality Metrics...\")\n",
    "\n",
    "quality_metrics = {\n",
    "    'total_rows': len(transform_df),\n",
    "    'total_columns': len(transform_df.columns),\n",
    "    'missing_values_count': transform_df.isnull().sum().sum(),\n",
    "    'duplicate_rows': transform_df.duplicated().sum(),\n",
    "    'numeric_columns': len(transform_df.select_dtypes(include=[np.number]).columns),\n",
    "    'categorical_columns': len(transform_df.select_dtypes(include=['object']).columns),\n",
    "    'unique_departments': transform_df['department'].nunique(),\n",
    "    'unique_age_groups': transform_df['age_group'].nunique(),\n",
    "    'unique_salary_categories': transform_df['salary_category'].nunique(),\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"Data Quality Metrics:\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(\"Saving Transformed data csv to: '/opt/ml/processing/output/transformed_data.csv' ...\")\n",
    "transform_df.to_csv(\"/opt/ml/processing/output/transformed_data.csv\", index=False)\n",
    "print(\"\\nTransformed data csv saved to: '/opt/ml/processing/output/transformed_data.csv'\")\n",
    "\n",
    "### Step 2: Save Department Statistics\n",
    "print(\"Saving department statistics...\")\n",
    "department_summary_report.to_csv(\"/opt/ml/processing/output/department_statistics.csv\", index=False)\n",
    "print(\"‚úÖ Department statistics saved to: '/opt/ml/processing/output/department_statistics.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cef80-1465-4bf7-a5dd-77c98d726590",
   "metadata": {},
   "source": [
    "#### Step 2: üèÉ‚Äç‚ôÇÔ∏è Execute SageMaker Processing Job\n",
    "Now we will execute the SageMaker Processing Job using the script created in the previous step. This job will process the data, apply transformations, and generate output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a8314a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:04:45.087685Z",
     "iopub.status.busy": "2025-07-20T23:04:45.087313Z",
     "iopub.status.idle": "2025-07-20T23:10:09.439006Z",
     "shell.execute_reply": "2025-07-20T23:10:09.438290Z",
     "shell.execute_reply.started": "2025-07-20T23:04:45.087663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2025-07-20-23-04-45-137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............\u001b[34m‚úÖ Dataset loaded successfully!\u001b[0m\n",
      "\u001b[34müìè Dataset shape: (20000, 8)\u001b[0m\n",
      "\u001b[34müìä Missing Value Patterns:\u001b[0m\n",
      "\u001b[34mMissing Age values:\n",
      "       age   salary department\u001b[0m\n",
      "\u001b[34m44     NaN  60000.0  Marketing\u001b[0m\n",
      "\u001b[34m115    NaN  60000.0         IT\u001b[0m\n",
      "\u001b[34m127    NaN      NaN  Marketing\u001b[0m\n",
      "\u001b[34m147    NaN  60000.0         HR\u001b[0m\n",
      "\u001b[34m164    NaN  70000.0         IT\u001b[0m\n",
      "\u001b[34m...    ...      ...        ...\u001b[0m\n",
      "\u001b[34m19872  NaN  60000.0         HR\u001b[0m\n",
      "\u001b[34m19921  NaN      NaN         HR\u001b[0m\n",
      "\u001b[34m19940  NaN  70000.0        NaN\u001b[0m\n",
      "\u001b[34m19997  NaN  60000.0         IT\u001b[0m\n",
      "\u001b[34m19998  NaN  60000.0  Marketing\u001b[0m\n",
      "\u001b[34m[1000 rows x 3 columns]\u001b[0m\n",
      "\u001b[34mMissing Salary values\n",
      "        age  salary department\u001b[0m\n",
      "\u001b[34m5      35.0     NaN         IT\u001b[0m\n",
      "\u001b[34m11     61.0     NaN         IT\u001b[0m\n",
      "\u001b[34m13     46.0     NaN        NaN\u001b[0m\n",
      "\u001b[34m14     48.0     NaN         IT\u001b[0m\n",
      "\u001b[34m15     61.0     NaN         HR\u001b[0m\n",
      "\u001b[34m...     ...     ...        ...\u001b[0m\n",
      "\u001b[34m19984  71.0     NaN        NaN\u001b[0m\n",
      "\u001b[34m19988  72.0     NaN  Marketing\u001b[0m\n",
      "\u001b[34m19992  60.0     NaN        NaN\u001b[0m\n",
      "\u001b[34m19993  76.0     NaN  Marketing\u001b[0m\n",
      "\u001b[34m19999  47.0     NaN        NaN\u001b[0m\n",
      "\u001b[34m[6481 rows x 3 columns]\u001b[0m\n",
      "\u001b[34mAge Median 48.0\u001b[0m\n",
      "\u001b[34mSalary Median 60000.0\u001b[0m\n",
      "\u001b[34mMissing values in each column\u001b[0m\n",
      "\u001b[34mPrint the missing values for Department\u001b[0m\n",
      "\u001b[34mMissing Department Missing values\n",
      "        age   salary department\u001b[0m\n",
      "\u001b[34m3      36.0  70000.0        NaN\u001b[0m\n",
      "\u001b[34m13     46.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m49     34.0  50000.0        NaN\u001b[0m\n",
      "\u001b[34m53     33.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m57     28.0  70000.0        NaN\u001b[0m\n",
      "\u001b[34m...     ...      ...        ...\u001b[0m\n",
      "\u001b[34m19973  50.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m19975  29.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m19984  71.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m19992  60.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m19999  47.0  60000.0        NaN\u001b[0m\n",
      "\u001b[34m[3997 rows x 3 columns]\u001b[0m\n",
      "\u001b[34mMissing values in each column\u001b[0m\n",
      "\u001b[34mid               0\u001b[0m\n",
      "\u001b[34mname             0\u001b[0m\n",
      "\u001b[34mage              0\u001b[0m\n",
      "\u001b[34msalary           0\u001b[0m\n",
      "\u001b[34mhire_date     2035\u001b[0m\n",
      "\u001b[34mprofile       2021\u001b[0m\n",
      "\u001b[34mdepartment       0\u001b[0m\n",
      "\u001b[34mbonus         1985\u001b[0m\n",
      "\u001b[34mdtype: int64\u001b[0m\n",
      "\u001b[34mTop rows from profile column \u001b[0m\n",
      "\u001b[34m0    {\"address\": \"Street 56, City 25\", \"phone\": \"39...\u001b[0m\n",
      "\u001b[34m1    {\"address\": \"Street 51, City 16\", \"phone\": \"56...\u001b[0m\n",
      "\u001b[34m2    {\"address\": \"Street 36, City 27\", \"phone\": \"30...\u001b[0m\n",
      "\u001b[34m3    {\"address\": \"Street 43, City 42\", \"phone\": \"16...\u001b[0m\n",
      "\u001b[34m4    {\"address\": \"Street 85, City 42\", \"phone\": \"90...\u001b[0m\n",
      "\u001b[34mName: profile, dtype: object\u001b[0m\n",
      "\u001b[34mProfile column values current data type\u001b[0m\n",
      "\u001b[34m<class 'str'>\u001b[0m\n",
      "\u001b[34mExtract Address Field....\u001b[0m\n",
      "\u001b[34mTop rows from profile column \u001b[0m\n",
      "\u001b[34m0    {'address': 'Street 56, City 25', 'phone': '39...\u001b[0m\n",
      "\u001b[34m1    {'address': 'Street 51, City 16', 'phone': '56...\u001b[0m\n",
      "\u001b[34m2    {'address': 'Street 36, City 27', 'phone': '30...\u001b[0m\n",
      "\u001b[34m3    {'address': 'Street 43, City 42', 'phone': '16...\u001b[0m\n",
      "\u001b[34m4    {'address': 'Street 85, City 42', 'phone': '90...\u001b[0m\n",
      "\u001b[34mName: profile, dtype: object\u001b[0m\n",
      "\u001b[34mTop rows from newly created address column \u001b[0m\n",
      "\u001b[34m0    Street 56, City 25\u001b[0m\n",
      "\u001b[34m1    Street 51, City 16\u001b[0m\n",
      "\u001b[34m2    Street 36, City 27\u001b[0m\n",
      "\u001b[34m3    Street 43, City 42\u001b[0m\n",
      "\u001b[34m4    Street 85, City 42\u001b[0m\n",
      "\u001b[34mName: address, dtype: object\u001b[0m\n",
      "\u001b[34mExtract Phone Field....\u001b[0m\n",
      "\u001b[34mTop rows from profile column \u001b[0m\n",
      "\u001b[34m0    {'address': 'Street 56, City 25', 'phone': '39...\u001b[0m\n",
      "\u001b[34m1    {'address': 'Street 51, City 16', 'phone': '56...\u001b[0m\n",
      "\u001b[34m2    {'address': 'Street 36, City 27', 'phone': '30...\u001b[0m\n",
      "\u001b[34m3    {'address': 'Street 43, City 42', 'phone': '16...\u001b[0m\n",
      "\u001b[34m4    {'address': 'Street 85, City 42', 'phone': '90...\u001b[0m\n",
      "\u001b[34mName: profile, dtype: object\u001b[0m\n",
      "\u001b[34mTop rows from newly created phone column \u001b[0m\n",
      "\u001b[34m0    3963547901\u001b[0m\n",
      "\u001b[34m1    5625259615\u001b[0m\n",
      "\u001b[34m2    3090408153\u001b[0m\n",
      "\u001b[34m3    1613259762\u001b[0m\n",
      "\u001b[34m4    9024436213\u001b[0m\n",
      "\u001b[34mName: phone, dtype: object\u001b[0m\n",
      "\u001b[34mExtract Email Field....\u001b[0m\n",
      "\u001b[34mTop rows from profile column \u001b[0m\n",
      "\u001b[34m0    {'address': 'Street 56, City 25', 'phone': '39...\u001b[0m\n",
      "\u001b[34m1    {'address': 'Street 51, City 16', 'phone': '56...\u001b[0m\n",
      "\u001b[34m2    {'address': 'Street 36, City 27', 'phone': '30...\u001b[0m\n",
      "\u001b[34m3    {'address': 'Street 43, City 42', 'phone': '16...\u001b[0m\n",
      "\u001b[34m4    {'address': 'Street 85, City 42', 'phone': '90...\u001b[0m\n",
      "\u001b[34mName: profile, dtype: object\u001b[0m\n",
      "\u001b[34mTop rows from newly created email column \u001b[0m\n",
      "\u001b[34m0    email_403@example.com\u001b[0m\n",
      "\u001b[34m1    email_825@example.com\u001b[0m\n",
      "\u001b[34m2    email_385@example.com\u001b[0m\n",
      "\u001b[34m3    email_402@example.com\u001b[0m\n",
      "\u001b[34m4    email_400@example.com\u001b[0m\n",
      "\u001b[34mName: email, dtype: object\u001b[0m\n",
      "\u001b[34m‚úÖ Profile fields extracted:\u001b[0m\n",
      "\u001b[34mColumns before dropping profile:\u001b[0m\n",
      "\u001b[34m['id', 'name', 'age', 'salary', 'hire_date', 'profile', 'department', 'bonus', 'address', 'phone', 'email']\u001b[0m\n",
      "\u001b[34mColumns in new DataFrame after dropping profile:\u001b[0m\n",
      "\u001b[34m['id', 'name', 'age', 'salary', 'hire_date', 'department', 'bonus', 'address', 'phone', 'email']\u001b[0m\n",
      "\u001b[34müíæ Saving cleaned data to: 'data/cleaned_data.csv' ...\u001b[0m\n",
      "\u001b[34m‚úÖ Cleaned data saved to: '/opt/ml/processing/output/cleaned_data.csv'\u001b[0m\n",
      "\u001b[34müîß Creating Address Length Feature...\u001b[0m\n",
      "\u001b[34mAddress followed by Address Length columns\u001b[0m\n",
      "\u001b[34müîß Creating Salary Categories...\u001b[0m\n",
      "\u001b[34mSample data after adding the 'salary_category' column: \u001b[0m\n",
      "\u001b[34müîß Creating Age Groups...\u001b[0m\n",
      "\u001b[34mAge group distribution:\u001b[0m\n",
      "\u001b[34mExperienced     7318\u001b[0m\n",
      "\u001b[34mSenior          4068\u001b[0m\n",
      "\u001b[34mEarly Career    3142\u001b[0m\n",
      "\u001b[34mMid Career      3022\u001b[0m\n",
      "\u001b[34mYoung           2450\u001b[0m\n",
      "\u001b[34mName: age_group, dtype: int64\u001b[0m\n",
      "\u001b[34mSample data after adding the 'age_group' column: \u001b[0m\n",
      "\u001b[34müîß Creating Department Statistics...\u001b[0m\n",
      "\u001b[34mSummary report of average salary and age based on the department:\n",
      "  Department  Average Salary  Average Age\u001b[0m\n",
      "\u001b[34m0    Finance    59830.035515    48.345256\u001b[0m\n",
      "\u001b[34m1         HR    60015.155342    48.620106\u001b[0m\n",
      "\u001b[34m2         IT    60034.499754    48.650074\u001b[0m\n",
      "\u001b[34m3  Marketing    60049.455984    48.419139\u001b[0m\n",
      "\u001b[34m4    Unknown    59939.954966    48.075056\u001b[0m\n",
      "\u001b[34müìä Data Quality Metrics...\u001b[0m\n",
      "\u001b[34mData Quality Metrics:\n",
      "  total_rows: 20000\n",
      "  total_columns: 13\n",
      "  missing_values_count: 10083\n",
      "  duplicate_rows: 0\n",
      "  numeric_columns: 6\n",
      "  categorical_columns: 5\n",
      "  unique_departments: 5\n",
      "  unique_age_groups: 5\n",
      "  unique_salary_categories: 2\n",
      "  processing_timestamp: 2025-07-20T23:07:04.957869\u001b[0m\n",
      "\u001b[34mSaving Transformed data csv to: '/opt/ml/processing/output/transformed_data.csv' ...\u001b[0m\n",
      "\u001b[34mTransformed data csv saved to: '/opt/ml/processing/output/transformed_data.csv'\u001b[0m\n",
      "\u001b[34mSaving department statistics...\u001b[0m\n",
      "\u001b[34m‚úÖ Department statistics saved to: '/opt/ml/processing/output/department_statistics.csv'\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_raw_data_prefix = \"input/\"\n",
    "output_preprocessed_data_prefix = \"output\"\n",
    "\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve('sklearn', 'us-east-1', '1.2-1'),\n",
    "    role=role,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.t3.medium',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code='preprocessing_script.py',\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"mock_data.csv\"),\n",
    "                            destination='/opt/ml/processing/input')], \n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                            destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"data-processed\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13cb859e-1854-4ae2-9bfa-f9ecf865bee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T23:15:20.158279Z",
     "iopub.status.busy": "2025-07-20T23:15:20.157495Z",
     "iopub.status.idle": "2025-07-20T23:15:20.428273Z",
     "shell.execute_reply": "2025-07-20T23:15:20.427473Z",
     "shell.execute_reply.started": "2025-07-20T23:15:20.158249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data..\n",
      "\n",
      "   id      name   age   salary   hire_date department   bonus  \\\n",
      "0   1  Name_103  77.0  60000.0  2023-01-28  Marketing  6437.0   \n",
      "1   2  Name_436  62.0  50000.0  2024-01-01  Marketing  6393.0   \n",
      "2   3  Name_861  61.0  60000.0         NaN         HR  9780.0   \n",
      "3   4  Name_271  36.0  70000.0  2018-01-07    Unknown  4262.0   \n",
      "4   5  Name_107  78.0  60000.0  2016-11-11         IT  8137.0   \n",
      "\n",
      "              address         phone                  email  \n",
      "0  Street 56, City 25  3.963548e+09  email_403@example.com  \n",
      "1  Street 51, City 16  5.625260e+09  email_825@example.com  \n",
      "2  Street 36, City 27  3.090408e+09  email_385@example.com  \n",
      "3  Street 43, City 42  1.613260e+09  email_402@example.com  \n",
      "4  Street 85, City 42  9.024436e+09  email_400@example.com  \n",
      "Trnasformed Data..\n",
      "\n",
      "   id      name   age   salary   hire_date department   bonus  \\\n",
      "0   1  Name_103  77.0  60000.0  2023-01-28  Marketing  6437.0   \n",
      "1   2  Name_436  62.0  50000.0  2024-01-01  Marketing  6393.0   \n",
      "2   3  Name_861  61.0  60000.0         NaN         HR  9780.0   \n",
      "3   4  Name_271  36.0  70000.0  2018-01-07    Unknown  4262.0   \n",
      "4   5  Name_107  78.0  60000.0  2016-11-11         IT  8137.0   \n",
      "\n",
      "              address         phone                  email  address_length  \\\n",
      "0  Street 56, City 25  3.963548e+09  email_403@example.com              18   \n",
      "1  Street 51, City 16  5.625260e+09  email_825@example.com              18   \n",
      "2  Street 36, City 27  3.090408e+09  email_385@example.com              18   \n",
      "3  Street 43, City 42  1.613260e+09  email_402@example.com              18   \n",
      "4  Street 85, City 42  9.024436e+09  email_400@example.com              18   \n",
      "\n",
      "  salary_category    age_group  \n",
      "0          medium  Experienced  \n",
      "1             low  Experienced  \n",
      "2          medium  Experienced  \n",
      "3          medium   Mid Career  \n",
      "4          medium  Experienced  \n",
      "Department Statistics Data..\n",
      "\n",
      "  Department  Average Salary  Average Age\n",
      "0    Finance    59830.035515    48.345256\n",
      "1         HR    60015.155342    48.620106\n",
      "2         IT    60034.499754    48.650074\n",
      "3  Marketing    60049.455984    48.419139\n",
      "4    Unknown    59939.954966    48.075056\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.read_csv(f's3://{bucket}/output/data-processed/cleaned_data.csv')\n",
    "df_department_statistics = pd.read_csv(f's3://{bucket}/output/data-processed/department_statistics.csv')\n",
    "df_transformed_data = pd.read_csv(f's3://{bucket}/output/data-processed/transformed_data.csv')\n",
    "print(\"Cleaned Data..\\n\")\n",
    "print(df_cleaned.head())\n",
    "print(\"Trnasformed Data..\\n\")\n",
    "print(df_transformed_data.head())\n",
    "print(\"Department Statistics Data..\\n\")\n",
    "print(df_department_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2d6f8-27a3-43ad-9e83-fdbf92a2fe23",
   "metadata": {},
   "source": [
    "## 4. Next Steps for MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab1b76-7e48-4105-8720-f35ed5584607",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T22:54:31.023163Z",
     "iopub.status.idle": "2025-07-20T22:54:31.025845Z",
     "shell.execute_reply": "2025-07-20T22:54:31.025626Z",
     "shell.execute_reply.started": "2025-07-20T22:54:31.025602Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\nüéØ Next Steps for MLOps:\")\n",
    "print(f\"  1. Model training using transformed features\")\n",
    "print(f\"  2. Model validation and testing\")\n",
    "print(f\"  3. Model deployment and monitoring\")\n",
    "print(f\"  4. Data drift monitoring using quality metrics\")\n",
    "print(f\"  5. Pipeline automation and orchestration\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ DATA TRANSFORMATION PIPELINE COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
